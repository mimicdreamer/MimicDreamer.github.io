<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training">
  <meta name="keywords" content="DriveDreamer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ms_icon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
    }  
      
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }  
    tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }  
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Haoyun Li<sup>1,2*</sup>,</span>
            <span class="author-block">
              Ivan Zhang<sup>1,3*</sup>,</span>
            <span class="author-block">
              Runqi Ouyang<sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=96lsfiUAAAAJ&hl">Xiaofeng Wang</a><sup>1,4*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl">Zheng Zhu</a><sup>1</sup>,</span>
            <span class="author-block">
              Zhiqin Yang<sup>1</sup>,</span>
            <span class="author-block">
              Zhentao Zhang<sup>2</sup>,</span>
            <span class="author-block">
              Boyuan Wang<sup>1,2</sup>,</span>
            <span class="author-block">
              Chaojun Ni<sup>1</sup>,</span>
            <span class="author-block">
              Wenkang Qin<sup>1</sup>,</span>
            <span class="author-block">
              Xinze Chen<sup>1</sup>,</span>
            <span class="author-block">
              Yun Ye<sup>1</sup>,</span>
            <span class="author-block">
              Guan Huang<sup>1</sup>,</span>
            <span class="author-block">
              Zhenbo Song<sup>3</sup>,</span>
            <span class="author-block">
              Xingang Wang<sup>2</sup></span>
            <br>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">GigaAI<sup>1</sup>,</span>&nbsp;
            <span class="author-block">CASIA<sup>2</sup>,</span>&nbsp;
            <span class="author-block">NJUST<sup>3</sup>,</span>&nbsp;
            <span class="author-block">Tsinghua University<sup>4</sup></span>&nbsp;
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2509.22199"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/GigaAI-research/MimicDreamer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/abs.png" style="width:100%;height:100%;">
       <p  style="font-size: 16px;"> 
         DriveDreamer excels in controllable driving video generation, aligning seamlessly with text prompts and structured traffic constraints. DriveDreamer can also interact with the driving scene and predict different future driving videos, based on input driving actions. Furthermore, DriveDreamer extends its utility to anticipate future driving actions.
      </p>
    </div>
  </div>
</section> -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training. For visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage. For viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping. For action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking. Empirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7% across six representative manipulation tasks.
          </p>
        </div> 
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/toc.png" style="width:100%;height:100%;">
          <p>
            Overview of <i>MimicDreamer</i>. Viewpoint branch (top left): egocentric videos are stabilized by <span style="font-variant: small-caps;">EgoStabilizer</span> (warp perspective + background inpainting) to produce stable egocentric videos. Camera intrinsics/extrinsics and the robot URDF drive sim rendering to generate additional stable ego views. Action branch (bottom left): 3D hand trajectories are converted to robot actions with IK solver. Visual alignment (right): <span style="font-variant: small-caps;">H2R Aligner</span> learns to bridge the human-to-robot visual gap using stable egocentric videos and simulation robot videos. The resulting synthesized robot videos and robot actions are used for VLA training.
          </p>
        </div>
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/h2r.png" style="width:100%;height:100%;">
          <p>
            <span style="font-variant: small-caps;">H2R Aligner</span>. During training, the real robot video V<sub>gt</sub>, background V<sub>scene</sub>, and simulated foreground V<sub>sim</sub> are encoded by a frozen VAE and channel-concatenated as [z̃<sub>tar</sub>, z<sub>scene</sub>, z<sub>sim</sub>] before entering the trainable H2R DiT, optimized with <code>CogVideoXLoss</code> loss. During inference, a hand-masked human background and IK-replayed simulation serve as conditions; the target starts from noise, is denoised by H2R DiT, and decoded by the frozen VAE into synthesized robot videos. 
          </p>
        </div>
      </div>
    </section>
  </div>
</section>


<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">Results of VLA policy on Mimic Robot Data</h2>
    <!-- <section class="hero method">
    <div class="container is-max-desktop">
    <div class="hero-body">   -->

  <h4 class="title">1. Few-Shot Experimental Results</h4>
    <div class="container is-max-desktop">
      <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/exp1.png" style="width:100%;height:100%;">
    </div> 
    <p>
      Quantitative Results Across Three Training Setups. SR and PSR for a Robot Only baseline (20 robot data), w. Minimal Robot trained primarily on synthesized data (20 human-to-robot data + 3 robot data), and w. Equal Data using a balanced mix (20 human-to-robot data + 20 robot data).
    </p>
    <br>
    <p style="margin-bottom: 30px;"></p>

	<h4 class="title">2. Scaling Experiment results</h4>
  <div class="container is-max-desktop">
    <div style="display: inline-block; width: 33%; vertical-align: top;">
      <img src="./static/images/Pick Bag.png" style="width:100%;">
      <p style="text-align: center;">(a) <code>Pick Bag</code></p>
    </div>
    <div style="display: inline-block; width: 33%; vertical-align: top;">
      <img src="./static/images/Clean Surface.png" style="width:100%;">
      <p style="text-align: center;">(b) <code>Clean Surface</code></p>
    </div>
    <div style="display: inline-block; width: 33%; vertical-align: top;">
      <img src="./static/images/Stack Bowls.png" style="width:100%;">
      <p style="text-align: center;">(c) <code>Stack Bowls</code></p>
    </div>
    
    <div style="display: inline-block; width: 33%; vertical-align: top;">
      <img src="./static/images/Dry Hands.png" style="width:100%;">
      <p style="text-align: center;">(d) <code>Dry Hands</code></p>
    </div>
    <div style="display: inline-block; width: 33%; vertical-align: top;">
      <img src="./static/images/Insert Tennis.png" style="width:100%;">
      <p style="text-align: center;">(e) <code>Insert Tennis</code></p>
    </div>
    <div style="display: inline-block; width: 33%; vertical-align: top;">
      <img src="./static/images/Stack Cups.png" style="width:100%;">
      <p style="text-align: center;">(f) <code>Stack Cups</code></p>
    </div>
  </div> 
  <p>
    Scaling Experiment Results. As more human-to-robot data is added, the <i>MimicDreamer</i>'s success rate monotonically increases across all six tasks.
  </p>
</section>

<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">Results of <span style="font-variant: small-caps;">H2R Aligner</span></h2>
    <!-- <section class="hero method">
    <div class="container is-max-desktop">
    <div class="hero-body">   -->
    <div class="container is-max-desktop">
      <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/exp3.png" style="width:100%;height:100%;">
    </div> 
    <p>
      Visual Results of <span style="font-variant: small-caps;">H2R Aligner</span>. Top: original human demonstration video. Middle: replayed robot simulation from the same action trajectories. Bottom: synthesized robot-domain video generated by <span style="font-variant: small-caps;">H2R Aligner</span>. The generated sequences transfer human motions into robot-arm appearances while preserving background context and manipulation semantics.
    </p>
</section>

<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">Results of <span style="font-variant: small-caps;">EgoStabilizer</span></h2>
    <!-- <section class="hero method"> -->
    <!-- <div class="container is-max-desktop">
    <div class="hero-body">   -->
    <div class="container is-max-desktop">
      <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/exp4.png" style="width:100%;height:100%;">
    </div> 
    <p>
      Per-category, frame-weighted means. “&darr;” lower is better; “&uarr;” higher is better.
      Cells show {before} &rarr; <strong>after</strong> (relative &Delta;%).
    </p>
    <div class="container is-max-desktop">
      <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/exp5.png" style="width:100%;height:100%;">
    </div> 
    <p>
      Qualitative evaluation of <span style="font-variant: small-caps;">EgoStabilizer</span>. On a 300-frame Clean Surface video, frames at indices 0, 150, and 300 are shown before and after stabilization. Keypoints such as wall corners and table–image intersections exhibit large jitter in the original video, whereas the stabilized outputs show negligible displacement, confirming effective viewpoint stabilization.
    </p>
</section>


<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">More Visual Results</h2> 
      <section class="hero method">
        <!-- <div class="container is-max-desktop"> -->
        <!-- <div class="hero-body">   -->
    
      <h4 class="title">1. Real Robot Replays</h4>
        <div class="container is-max-desktop">
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="32%"> 
            <source src="./static/videos/real/Pick up a Bag.mp4"
            type="video/mp4">
          </video>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="32%"> 
            <source src="./static/videos/real/Clean Surface.mp4"
            type="video/mp4">
          </video>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="32%"> 
            <source src="./static/videos/real/Stack bowls.mp4"
            type="video/mp4">
          </video>
          <br>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="32%"> 
            <source src="./static/videos/real/Dry Hands.mp4"
            type="video/mp4">
          </video>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="32%"> 
            <source src="./static/videos/real/Stack Tennis.mp4"
            type="video/mp4">
          </video>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="32%"> 
            <source src="./static/videos/real/Stack Cups.mp4"
            type="video/mp4">
          </video>
      </div> 
        <p style="margin-bottom: 30px;"></p>
        <h4 class="title">2. <span style="font-variant: small-caps;">H2R Aligner</span></h4>
        <div class="container is-max-desktop">
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="48%"> 
            <source src="./static/videos/h2r/out_0.mp4"
            type="video/mp4">
          </video>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="48%"> 
            <source src="./static/videos/h2r/out_1.mp4"
            type="video/mp4">
          </video>
          <br>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="48%"> 
            <source src="./static/videos/h2r/out_2.mp4"
            type="video/mp4">
          </video>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="48%"> 
            <source src="./static/videos/h2r/out_3.mp4"
            type="video/mp4">
          </video>
      </div> 
      <p style="margin-bottom: 30px;"></p>
        <h4 class="title">3. <span style="font-variant: small-caps;">EgoStabilizer</span></h4>
        <div class="container is-max-desktop">
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="48%"> 
            <source src="./static/videos/stable/1_contrast.mp4"
            type="video/mp4">
          </video>
          <video preload="auto"poster="" id="tree" autoplay controls muted loop width="48%"> 
            <source src="./static/videos/stable/61_contrast.mp4"
            type="video/mp4">
          </video>
      </div> 
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p> If you use our work in your research, please cite: </p>
    <pre><code>Coming Soon</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/JeffWang987/DriveDreamer" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
